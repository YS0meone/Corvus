import uuid
from langchain_core.tools import tool
from app.agent.paper_finder_fast import paper_finder_fast_graph
from app.agent.paper_finder import paper_finder
from app.agent.qa import qa_graph
from langgraph.graph import StateGraph
from app.agent.states import SupervisorState
import logging
from app.core.config import settings
from app.core.logging_config import setup_logging
from langchain.chat_models import init_chat_model
from langgraph.graph import START, END
from pydantic import BaseModel, Field
from langchain.messages import SystemMessage, AIMessage, ToolMessage, HumanMessage
from app.tools.search import get_paper_details
from langgraph.types import Command, interrupt
from langchain.tools import ToolRuntime
from langgraph.graph.ui import push_ui_message
from langgraph.prebuilt import ToolNode, tools_condition
from typing import List, Literal
from app.agent.ui_manager import UIManager
from app.core.schema import StepName, StepStatus
from app.agent.utils import get_paper_abstract

DO_NOT_RENDER_ID_PREFIX = "do-not-render-"

setup_logging()

logger = logging.getLogger(__name__)


async def _optimize_for_paper_search(messages: list) -> tuple[str, str]:
    """
    Generate two complementary representations of the user's paper-finding intent:
    - search_task: natural-language task description for the planner and search agent
    - rerank_query: keyword-style query used only for semantic reranking
    """
    class SearchOptimizationOutput(BaseModel):
        search_task: str = Field(
            description=(
                "A self-contained natural-language task for a research assistant. "
                "Describe exactly what papers to find in plain English. "
                "Resolve all pronouns and vague references using prior conversation context. "
                "Use complete sentences. "
                "Example: 'Find the PaperQA paper by Lala et al., a question-answering "
                "system designed for scientific literature retrieval.'"
            )
        )
        rerank_query: str = Field(
            description=(
                "A keyword-style query for semantic relevance scoring against paper abstracts. "
                "5-15 words. Include core concepts, method names, domain terms, "
                "and any paper/author names. Do NOT write a full sentence. "
                "Example: 'PaperQA question answering scientific literature Lala 2023'"
            )
        )

    system = SystemMessage(content=(
        "You are an expert at analysing research requests. "
        "Given the conversation history, produce two outputs: "
        "(1) a natural-language task description so a research planner knows exactly what to do, "
        "and (2) a keyword query for semantic reranking of candidate papers. "
        "Resolve all references (e.g. 'that paper', 'it', 'their method') using context from earlier messages. "
        "For the search_task: faithfully reflect only what the user asked for. "
        "Do NOT add sub-topics, concepts, or angles the user did not mention — "
        "the planner will over-engineer the search plan if given extra scope."
    ))
    structured = supervisor_model.bind_tools(
        [SearchOptimizationOutput], tool_choice="SearchOptimizationOutput"
    )
    msg = await structured.ainvoke([system] + messages)
    args = msg.tool_calls[0]["args"]
    return args["search_task"], args["rerank_query"]


async def _optimize_for_qa(messages: list) -> str:
    """Derive a fully self-contained research question from the full conversation history."""
    class QAQueryOutput(BaseModel):
        qa_query: str = Field(
            description=(
                "A fully self-contained research question. "
                "All pronouns and vague references must be replaced with explicit names "
                "using context from earlier messages. "
                "The question must make complete sense without any prior context."
            )
        )

    system = SystemMessage(content=(
        "You are an expert at reformulating research questions. "
        "Given the conversation history below, rewrite the user's latest question "
        "as a fully self-contained research question. "
        "Resolve all references (e.g. 'that paper', 'it', 'their method', 'the above') "
        "using context from earlier messages."
    ))
    structured = supervisor_model.bind_tools([QAQueryOutput], tool_choice="QAQueryOutput")
    msg = await structured.ainvoke([system] + messages)
    return msg.tool_calls[0]["args"]["qa_query"]


@tool
async def find_papers(runtime: ToolRuntime) -> Command:
    """
    Find papers using the optimized query generated by the previous workflow.
    It would update the current papers list with the new papers found.
    Trust the result from the tools would find the most relevant papers.
    """

    messages = runtime.state.get("messages", [])
    papers = runtime.state.get("papers", [])

    search_task, rerank_query = await _optimize_for_paper_search(messages)
    logger.info("Finding papers — task: %s", search_task[:120])
    logger.info("Rerank query: %s", rerank_query)
    logger.debug("Current paper count: %d", len(papers))

    state = {
        "search_task": search_task,
        "rerank_query": rerank_query,
        "papers": papers,
        "messages": [HumanMessage(content=search_task)],
    }
    result = await paper_finder.ainvoke(state)
    papers = result["papers"]

    logger.info("Found %d papers", len(papers))

    return Command(
        update={"papers": result["papers"],
        "messages": [ToolMessage(content=f"I found {len(result['papers'])} papers for your query.", tool_call_id=runtime.tool_call_id)]
        })

@tool
async def retrieve_and_answer_question(runtime: ToolRuntime) -> str:
    """
    Answer the user question based on the current papers.
    The tool would retrieve evidence from the selected papers and answer the user question based on the evidence.
    Return:
    - The answer to the user's question
    """
    messages = runtime.state.get("messages", [])
    papers = runtime.state.get("papers", [])
    selected_ids = runtime.state.get("selected_paper_ids", [])

    user_query = await _optimize_for_qa(messages)
    logger.info(f"Answering question for query: {user_query[:100]}{'...' if len(user_query) > 100 else ''}")
    logger.debug(f"Selected paper count: {len(selected_ids)}, Total papers: {len(papers)}")

    if not selected_ids:
        logger.warning("No papers selected for QA")
        return "No papers selected for QA. Please select papers first."

    qa_state = {
        "messages": [HumanMessage(content=user_query)],
        "papers": papers,
        "selected_paper_ids": selected_ids,
        "user_query": user_query,
    }

    logger.debug("Invoking QA graph")
    result = await qa_graph.ainvoke(qa_state)
    logger.info("QA graph completed successfully")
    return result["final_answer"]


supervisor_model = init_chat_model(model=settings.SUPERVISOR_MODEL_NAME)
tools = [find_papers, get_paper_details, retrieve_and_answer_question]
supervisor_tool_node = ToolNode(tools)


# ─── Query evaluation ─────────────────────────────────────────────────────────

async def query_evaluation(state: SupervisorState):
    logger.debug("Query evaluation node invoked")
    new_ui_tracking_message = AIMessage(id=str(uuid.uuid4()), content="")
    new_ui_tracking_id = str(uuid.uuid4())
    new_ui_state = {
        "steps": [],
        "ui_tracking_message": new_ui_tracking_message,
        "ui_tracking_id": new_ui_tracking_id
    }
    ui_manager = UIManager.from_state(new_ui_state)
    ui_manager.update_ui(StepName.QUERY_EVALUATION, StepStatus.RUNNING)

    papers = state.get("papers", [])
    selected_ids = state.get("selected_paper_ids", [])
    papers_text = _format_papers_for_prompt(papers, selected_ids)

    system_prompt = f"""You are a query evaluator for Corvus, an AI research assistant that helps users discover and understand academic papers.

## What Corvus can do
1. **Find papers** — search Semantic Scholar for academic papers on a research topic
2. **Answer questions** — retrieve evidence from user-selected papers and answer scientific questions

## Papers currently in context ({len(papers)} total — [SELECTED] = chosen for QA)
{papers_text}

## Your task
Evaluate the latest user message and return one of five decisions:

- **clear** — the query is valid and specific enough for the system to handle. Proceed.
- **needs_clarification** — the query is relevant (paper search or scientific Q&A) but too vague to act on.
  Example: "find papers about AI", "tell me about transformers", "search ML papers".
  Ask the user to be more specific (topic, methods, time period, authors, etc.). Be friendly, not critical.
- **unselected_paper** — the user is asking a question about a paper that is visible in the context list above but NOT marked [SELECTED]. Remind them to select it first.
- **irrelevant** — the query has nothing to do with paper discovery or scientific Q&A (e.g. coding help, weather, casual chat). Briefly explain what Corvus does and invite them to try a research query.
- **inappropriate** — the query is offensive or harmful. Politely decline.

## Important guidance
- Use the full conversation history to understand context. A short follow-up like "what about attention?" can be **clear** if prior messages establish the topic.
- Do NOT be overly strict. "Find papers on attention mechanisms in transformers" is clear even without exact author names.
- Only flag **needs_clarification** when the topic is genuinely too broad to search meaningfully.
- For **unselected_paper**: only flag this when the user's question clearly targets a specific paper shown in the context list that isn't selected.
- Always write `response` in second person, addressing the user directly and warmly.
"""

    class ClarificationOutput(BaseModel):
        reasoning: str = Field(description="Brief internal reasoning for the decision")
        decision: Literal["clear", "needs_clarification", "unselected_paper", "irrelevant", "inappropriate"] = Field(
            description="The evaluation outcome"
        )
        response: str = Field(
            default="",
            description=(
                "Message to show the user. Required for all non-clear decisions. "
                "Leave empty for 'clear'."
            )
        )

    structured_model = supervisor_model.bind_tools([ClarificationOutput], tool_choice="ClarificationOutput")
    msg = await structured_model.ainvoke([
        SystemMessage(content=system_prompt)
    ] + state["messages"])

    result = msg.tool_calls[0]["args"]
    decision = result.get("decision", "clear")
    response_text = result.get("response", "")
    logger.info(f"Query evaluation decision='{decision}' — {result.get('reasoning', '')}")

    if decision == "clear":
        new_steps = ui_manager.update_ui(StepName.QUERY_EVALUATION, StepStatus.COMPLETED, decision=decision)
        return {
            "messages": [new_ui_tracking_message],
            "is_clear": True,
            "steps": new_steps,
            "ui_tracking_message": new_ui_tracking_message,
            "ui_tracking_id": new_ui_tracking_id
        }
    else:
        new_steps = ui_manager.update_ui(StepName.QUERY_EVALUATION, StepStatus.COMPLETED, decision=decision)
        return {
            "messages": [new_ui_tracking_message, AIMessage(content=response_text)],
            "is_clear": False,
            "steps": new_steps,
            "ui_tracking_message": new_ui_tracking_message,
            "ui_tracking_id": new_ui_tracking_id
        }


def should_proceed(state: SupervisorState):
    is_clear = state.get("is_clear", True)
    return "planner" if is_clear else "end"


# ─── Planner ──────────────────────────────────────────────────────────────────

class PlanOutput(BaseModel):
    reasoning: str = Field(description="Brief reasoning for the chosen plan")
    plan: Literal["find_only", "qa_only", "find_then_qa"] = Field(
        description=(
            "find_only: search for new papers only. "
            "qa_only: answer a question using the papers already in context. "
            "find_then_qa: search for papers first, then answer the question."
        )
    )


# Maps the 3 plan labels to the ordered action lists used by the executor
_PLAN_TO_STEPS: dict[str, list[str]] = {
    "find_only":    ["find_papers"],
    "qa_only":      ["retrieve_and_answer_question"],
    "find_then_qa": ["find_papers", "retrieve_and_answer_question"],
}


def _format_papers_for_prompt(papers: list, selected_ids: list[str]) -> str:
    """Render papers with title, authors, year, and a truncated abstract."""
    if not papers:
        return "  (none yet)"
    lines = []
    for p in papers:
        selected_marker = "[SELECTED] " if p.paperId in selected_ids else ""
        title = p.title or "Untitled"
        year = f" ({p.year})" if p.year else ""
        authors = ""
        if p.authors:
            names = [a.get("name", "") for a in p.authors[:3] if isinstance(a, dict)]
            authors = f" — {', '.join(n for n in names if n)}"
            if len(p.authors) > 3:
                authors += " et al."
        abstract = ""
        if p.abstract:
            abstract = "\n    Abstract: " + p.abstract[:200].replace("\n", " ")
            if len(p.abstract) > 200:
                abstract += "..."
        lines.append(f"  {selected_marker}{title}{year}{authors}{abstract}")
    return "\n".join(lines)


async def planner(state: SupervisorState):
    """Decide the plan for the current user turn using one of three fixed labels."""
    ui_manager = UIManager.from_state(state)
    logger.info("Planner node invoked")
    ui_manager.update_ui(StepName.PLAN, StepStatus.RUNNING)

    papers = state.get("papers", [])
    selected_ids = state.get("selected_paper_ids", [])
    papers_text = _format_papers_for_prompt(papers, selected_ids)

    plan_prompt = f"""You are a planner for a research assistant system.
The query has already been validated as research-related by a prior node, so it always concerns finding or understanding academic papers.

## Your task
Choose exactly one plan label based on the user's intent:

- **find_only** — the user wants to search for new papers (no question to answer yet)
- **qa_only** — the user wants to ask a question about papers that are already in context
- **find_then_qa** — the user wants to find papers AND get a question answered about them

## Decision guidance
- Use **qa_only** when the user is asking about the papers that is currently in context.
- Use **find_only** when the user is only interested in finding papers that is not in the current context and the user does not have any follow up questions.
- Use **find_then_qa** when the user asks a question about papers that is not in the current context.
- Use the full conversation history below to resolve references like "these papers", "that method", "the authors mentioned".

## Papers currently in context ({len(papers)} total — [SELECTED] = chosen for QA)
{papers_text}
"""

    structured = supervisor_model.bind_tools([PlanOutput], tool_choice="PlanOutput")
    response = await structured.ainvoke([
        SystemMessage(content=plan_prompt),
        *state.get("messages", []),
    ])

    plan_result = response.tool_calls[0]["args"]
    plan_label = plan_result.get("plan", "find_only")
    reasoning = plan_result.get("reasoning", "")
    logger.info(f"Planner chose plan='{plan_label}' — {reasoning}")

    steps = _PLAN_TO_STEPS[plan_label]

    new_steps = ui_manager.update_ui(StepName.PLAN, StepStatus.COMPLETED, plan_label)
    return {"plan_steps": steps, "steps": new_steps}


# ─── Executor ─────────────────────────────────────────────────────────────────

def executor(state: SupervisorState):
    """Pop the first step from the plan and issue the corresponding tool call."""
    ui_manager = UIManager.from_state(state)
    plan = list(state.get("plan_steps", []))

    if not plan:
        logger.info("Plan is empty — executor producing final end message")
        # No tool call → tools_condition will route to __end__
        return {"messages": [AIMessage(content="")], "plan_steps": []}

    current_step = plan.pop(0)
    logger.info(f"Executor dispatching step: {current_step}")

    if current_step == "find_papers":
        ui_manager.update_ui(StepName.FIND_PAPERS, StepStatus.RUNNING)
    elif current_step == "retrieve_and_answer_question":
        ui_manager.update_ui(StepName.RETRIEVE_AND_ANSWER_QUESTION, StepStatus.RUNNING)

    tool_call = {
        "name": current_step,
        "args": {},
        "id": str(uuid.uuid4()),
        "type": "tool_call",
    }
    return {
        "messages": [AIMessage(content="", tool_calls=[tool_call])],
        "plan_steps": plan,
        "steps": ui_manager.steps,
    }


# ─── Post-tool node ───────────────────────────────────────────────────────────
# Runs immediately after supervisor_tools. Commits UI messages and answer to
# state so they are checkpointed BEFORE any interrupt is issued.

def post_tool(state: SupervisorState):
    """
    Commit tool-result side-effects to state (paper list UI, final answer).
    This node always completes normally so its return value is checkpointed.
    """
    ui_manager = UIManager.from_state(state)
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None

    if not isinstance(last_message, ToolMessage):
        logger.warning("post_tool called but last message is not a ToolMessage")
        return {}

    last_tool = last_message.name
    logger.info(f"post_tool handling result of: {last_tool}")

    if last_tool == "find_papers":
        papers = state.get("papers", [])
        logger.info(f"find_papers completed — {len(papers)} papers — pushing paper list UI")

        paper_list_ui_message = AIMessage(id=str(uuid.uuid4()), content="")
        papers_data = [p.model_dump() if hasattr(p, "model_dump") else p for p in papers]
        push_ui_message(
            "papers",
            {"papers": papers_data},
            message=paper_list_ui_message,
        )
        ui_manager.update_ui(StepName.FIND_PAPERS, StepStatus.COMPLETED, len(papers))
        # paper_list_ui_message is committed to state here (node returns normally)
        return {"messages": [paper_list_ui_message], "steps": ui_manager.steps}

    elif last_tool == "retrieve_and_answer_question":
        logger.info("retrieve_and_answer_question completed — emitting final answer")
        ui_manager.update_ui(StepName.RETRIEVE_AND_ANSWER_QUESTION, StepStatus.COMPLETED)
        answer = last_message.content
        return {
            "plan_steps": [],
            "messages": [AIMessage(content=answer)],
            "steps": ui_manager.steps,
        }

    else:
        logger.warning(f"post_tool: unknown tool {last_tool}")
        return {}


def post_tool_route(state: SupervisorState) -> str:
    """Route to replanner only when find_papers ran AND QA is still in the plan."""
    messages = state.get("messages", [])
    for msg in reversed(messages):
        if isinstance(msg, ToolMessage):
            if msg.name == "find_papers":
                remaining = state.get("plan_steps", [])
                if "retrieve_and_answer_question" in remaining:
                    return "replanner"
                return "__end__"
            return "__end__"
    return "__end__"


# ─── Replanner ────────────────────────────────────────────────────────────────
# Runs only after find_papers. Interrupts so the user can select papers, then
# re-evaluates the remaining plan on resume.
# When LangGraph resumes from an interrupt it re-runs this node from the top;
# interrupt() returns the resume value immediately on the second execution.

async def replanner(state: SupervisorState):
    """Interrupt for paper selection, then deterministically update the plan.

    The resume payload is a dict sent by the client:
        { "selected_paper_ids": [...], "user_message": "..." | null }

    selected_paper_ids and the user message cannot be passed as a state update
    alongside a Command(resume=...) — the LangGraph server treats `input` and
    `command` as mutually exclusive.  Embedding them in the resume value is the
    reliable way to carry them across the interrupt boundary.
    """
    # Pause here — paper list UI is already in state (committed by post_tool).
    resume_payload = interrupt("select_papers")

    # ── Resumed ──────────────────────────────────────────────────────────────
    logger.info(f"Replanner resumed, payload: {resume_payload}")

    # Extract selected_paper_ids and optional user message from the resume payload.
    if isinstance(resume_payload, dict):
        selected_ids = resume_payload.get("selected_paper_ids", [])
        user_message_text = resume_payload.get("user_message") or None
    else:
        # Fallback: plain string resume (e.g. "continue")
        selected_ids = state.get("selected_paper_ids", [])
        user_message_text = str(resume_payload) if resume_payload not in (None, "continue", "") else None

    logger.info(f"Replanner: selected_ids={selected_ids}, user_message={user_message_text!r}")

    # Build the updated messages list (include the human message if provided)
    new_messages: list = []
    if user_message_text:
        new_messages.append(HumanMessage(content=user_message_text))

    # Rejection path: clear plan so the graph ends after executor finds nothing to do.
    if not selected_ids:
        return {
            "plan_steps": [],
            "selected_paper_ids": [],
            "messages": new_messages,
            "steps": state.get("steps", []),
        }

    # Acceptance path: keep existing remaining steps, commit selected paper IDs.
    remaining_steps = list(state.get("plan_steps", []))
    return {
        "plan_steps": remaining_steps,
        "selected_paper_ids": selected_ids,
        "messages": new_messages,
        "steps": state.get("steps", []),
    }


# ─── Graph assembly ───────────────────────────────────────────────────────────

graph = StateGraph(SupervisorState)
graph.add_node("query_evaluation", query_evaluation)
graph.add_node("planner", planner)
graph.add_node("executor", executor)
graph.add_node("supervisor_tools", supervisor_tool_node)
graph.add_node("post_tool", post_tool)
graph.add_node("replanner", replanner)

graph.add_edge(START, "query_evaluation")
graph.add_conditional_edges(
    "query_evaluation",
    should_proceed,
    {"planner": "planner", "end": END}
)
graph.add_edge("planner", "executor")
graph.add_conditional_edges(
    "executor",
    tools_condition,
    {"tools": "supervisor_tools", "__end__": END}
)
graph.add_edge("supervisor_tools", "post_tool")
graph.add_conditional_edges(
    "post_tool",
    post_tool_route,
    {"replanner": "replanner", "__end__": END}
)
graph.add_edge("replanner", "executor")

graph = graph.compile().with_config({"recursion_limit": 100})
